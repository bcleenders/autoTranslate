\section{Introduction}
The Internet offers a vast amount of natural language that can be used in natural language processing, for a very low price. A study by Buck et al.~\cite{buck2014n} estimated that each of the top 10 most frequently used languages on the internet has at least 250 GiB worth of text publically available online. For English (the most frequent), they even found 23 TiB of text.

Such amounts of text have a big potential to be used for the training of language models, but only if building these models can be done in an unsupervised fashion. Manually curating, marking and tagging text is far too much work to be feasible. With unsupervised algorithms, however, the structure in language can be exploited to let computers build language models.

\subsection{Problem Statement}
This project will focus on unsupervised training of computer models to translate words. Specifically, we focus on the use of word2vec~\cite{mikolov2013efficient} to provide translations.

Given the large bodies of text publically available, we want to provide automated translations. 

\subsection{Prior Work}
\label{sec:prior_work}
Since the introduction of word2vec~\cite{mikolov2013efficient, mikolov2013distributed} in 2013, the algorithm has seen a wide variety of usecases. In the initial paper~\cite{mikolov2013efficient}, Mikolov et. al describe interesting relations between vectors corresponding to words. 
A famous example of how word2vec models relations between words as mathematical equations is $king - man + woman = queen$.
The sematic relationships between man/woman and king/queen are preserved in the transformation of words to vectors, and can be expressed with basic algebra.

Subsequent papers have improved the algorithm both in terms of accuracy (\cite{levy2014linguistic}), performance, parallelization and extended the initial scope of applications. A good example of the latter is a paper by Boycheva~\cite{boycheva2015distributional}, which uses word2vec outside the natural language processing (NLP) domain but to generate playlists. Based on a set of playlists, their word2vec-based algorithm can suggest new playlists with artists that go well together.

One of the applications of word2vec inside the NLP domain, is exploiting similarities in languages for assistance in machine translation~\cite{wolf2014joint}. Mikolov et al.~\cite{mikolov2013exploiting} released a subsequent paper on word2vec, in which they describe similarities between models of different languages. An example they give, is how the usage of the numbers one to five in English is very similar to the usage in Spanish, and likewise for the names of animals. Figure~\ref{fig:english_spanish} shows a graphical representation of word vectors in English and Spanish.

\begin{figure}[ht!]
  \centering \includegraphics[width=\linewidth]{images/english_spanish}
  \caption{Vector representations of English and Spanish words, after dimensionality reduction and rotation. Notice the high level of similarity between both languages. Reprinted from Mikolov et al.~\cite{mikolov2013exploiting}}
  \label{fig:english_spanish}
\end{figure}

The similarities between languages can be used to predict translations for words without any human interaction or labeled input data. Using only unsupervised machine learning techniques, a computer could learn how to translate English to for instance Spanish and vice versa. The only requirement is a large amount of text in both languages to train the word2vec models on.

In this research, we will focus on this specific application of word2vec: using similarities in languages to provide translations of words.

It is important to note that word2vec only uses information of co-occurrences to model words. It does not learn grammatical concepts other than by statistical analysis. This limits our translation to single words; although the translator might be able to translate each word individually, it cannot learn that each finite verb must has a subject, that "we" is plural, etc. It will learn that "swim" is to "swimming" as "walk" is to "walking", but will not know that "swimming" is a gerund. Note that word2vec can be extended to sentences or whole documents as proposed by Le et al. \cite{le2014distributed} but this will be out of scope for our research.

\subsection{Research Purpose}
This research aims to improve the capabilities of machines to learn translations with minimal human intervention. Previous research~\cite{mikolov2013exploiting, wolf2014joint} has already shown potential for word2vec in the context of automatic translation, as discussed in section~\ref{sec:prior_work}.

However, we found no practical implementations using Word2Vec and no further research on different setups for word2vec based machine translation.

\section{Research goal}
The goal of this research is to give practical advice on a setup for word2vec based machine translation.

Because we aim at a practical purpose of our research, we will also release all code used in our experiments. We hope this makes our research so trivially reproducible that others will further develop and use our results.

\subsection{Tasks}
To build a test system and perform our experiments, we need to execute the following steps:

\begin{itemize}
  \item Build (at least) two working setups for automated translation:
    \begin{itemize}
      \item A single-model setup:
      \begin{itemize}
        \item Merge datasets, train single model with plain word2vec (easy)
        \item Build a translator on top of the model using the same relational structure as in the man/woman king/queen example.
      \end{itemize}
      \item A multi-model setup:
      \begin{itemize}
        \item Train the individual models
        \item Train transformation matrix between models, for example using linear regression.
        \item Build a translator that converts a word to a vector, "translates" the vector and looks up the corresponding word in the other model.
      \end{itemize}
    \end{itemize}
  \item Make a verification mechanism
  \item Verify which setup works better
\end{itemize}

\section{Planning}
A global overview of the milestones we defined in our research is as follows;
\begin{description}
  \item \emph{Half September - October: Literary Study)}\\
    During this phase, a list of relevant papers (e.g. \cite{levy2014linguistic, mikolov2013exploiting, wolf2014joint}) is collected, short-listed to a readable size and read. Each paper will get a brief (informal) summarization to capture the essence of the paper insofar as it is relevant to this research. These summaries help process the information and provide quick access during the practical research.\\

    Not all papers relevant to the research will be read during this phase, so next phases will include a fair share of reading.
  \item \emph{October: Preparing experiments}\\
    In this phase, we gather the required corpora (see Section~\ref{sec:resources}) and implement our translation programs.
  \item \emph{November: Running experiments}\\
    Running the experiments includes training the models (which might take several days) and running the translation programs against a set of predefined correct translations.
  \item \emph{Before 12 December: Incorporating results in paper}\\
    During this phase, we will incorporate the results obtained in the experiments into our final paper.
\end{description}

\section{Allocation of Responsibilities}
Since our datasets are too big to be processed on a laptop, a cluster is needed. This will be provided by SICS (Swedish Institute of Computes Science \footnote{\url{http://www.sics.se/}}). Bram will be responsible for downloading and cleaning the different resources of chapter \ref{sec:resources} plus uploading these to the cluster. Marc will be responsible for setting up the virtual machines in the cluster in order to run our experiments. This will involve installing the dependencies for word2vec to be able to run it in a distributed mode. When this is done we will run the experiments together.

\section{Resources}
\label{sec:resources}
We have identified four main resources needed for this research: a large English corpus, a large Dutch corpus, a large set of word translations from Dutch to English and computional resources to analyze the data.

The corpora for both languages should be several tens of gigabytes or more. Mikolov et al.~\cite{mikolov2013distributed} specifically state that a large amount of training data is crucial for word2vec to build a correct model. They speak of corpora 30 billion words, with a significant decrease in quality of the results when lowering the corpus size to only 6 billion words.

For this research, we will look at the following corpora:
\begin{itemize}
  \item \textit{Reddit corpus}
    \footnote{\url{https://archive.org/details/2015_reddit_comments_corpus}}
    (1TB uncompressed JSON, 50 billion words) Corpus containing all posts on Reddit. It is mostly English, but also contains a few other languages.
  \item \textit{Wikipedia EN crawl} 
    \footnote{\url{https://dumps.wikimedia.org/enwiki/20150901/}}
    (60GB uncompressed XML, 3 billion words) Corpus of all text on the English Wikipedia (ignoring revisions).
  \item \textit{Wikipedia NL Crawl} 
    \footnote{\url{https://dumps.wikimedia.org/nlwiki/20150901/}}
    (5GB uncompressed XML, 250 million words) Corpus of all text on the Dutch Wikipedia (ignoring revisions).
  \item \textit{SoNaR-500} 
    \footnote{\url{http://tst-centrale.org/producten/corpora/sonar-corpus/6-85}}
    (60GB uncompressed text, >500 million words) Curated corpus gathered by several Dutch universities, consisting of newspapers, subtitles, etc.
  \item \textit{Dutch Parallel Corpus} 
    \footnote{\url{http://tst-centrale.org/producten/corpora/dutch-parallel-corpus-niet-commercieel/6-65}}
    (10 million pairs of words Dutch-English.
\end{itemize}

These datasets are all open for academic usage, in various degrees of opennes. The Reddit corpus is available through their API or with bittorrent, the Wikipedia data is freely available for download under CC-BY-SA license, and both SoNaR-500 and the Dutch Parallel Corpus are available after signing a special research license agreement.

Before processing the corpora with word2vec, we will convert them to a universal format: plain text, with special characters (everything that is not alphanumeric, space or dash) removed, and one entity (post, article or article) per line.

To make our research reproducible, we will publish all our code online and provide lists with hashes of all our input and intermediate steps (e.g. after cleaning the data).