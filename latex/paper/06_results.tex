\section{Results}
\label{sec:results}

\todo{Add more results (matter of running more experiments, mostly)}

\subsection{Multi-model Translations}
Figure~\ref{fig:accuracy_multi_model_wikis} shows the accuracy of the multi-model translations, both for models trained on 100 and 400 dimensions. We used the same training set of translations to train the matrix on, and both language models are trained on respectively the Dutch and English Wikipedia.

An interesting observation is that the accuracy at all levels (top 1, top 5 and top 10) starts off lower for models trained at 400 dimensions. However, at the biggest training set, each level is better than the corresponding level at 100 dimensions. This suggests a form of overfitting; the translation matrix might be overfitted on the translation samples.

\todo{There's also a dip towards the end. Check if that's still there if we run it with differently split test/training data sets.}

\begin{figure}[ht!]
  \centering \includegraphics[width=\linewidth]{images/accuracy_multi_model_wikis}
  \caption{Accuracy for multi-model translations, trained on Dutch and English wiki. 'x' marks denote dimensionality 400, '+' marks dimensionalty 100.}
  \label{fig:accuracy_multi_model_wikis}
\end{figure}

<<<<<<< HEAD

\begin{figure}[ht!]
  \centering \includegraphics[width=\linewidth]{images/accuracy_nlwiki400_enwiki400_lowercase}
  \caption{Accuracy for different sizes of training dataset for translation matrix, language models trained on Wikipedia at 400 dimensions.}
  \label{fig:accuracy_nlwiki400_enwiki400_lowercase}
\end{figure}

\subsection{Single-model Translations}
\subsubsection{Using Relations}

\subsubsection{Using Translation Matrix}
=======
\subsection{Single-model Translations}
In this section, we use two models: both are trained on the text of the English Wikipedia and the Dutch Wikipedia combined. Since the Dutch Wikipedia is much smaller than its English counterpart (by a factor of 8, see table~\ref{table:datasets}), we ran it eight times over the Dutch text. This artificially increases the weight given to Dutch text, so both are equally well represented.

The only difference between the two models is the number of dimensions: one is trained at 100, and the other at 400 dimensions.

\subsubsection{Using Relations}
The method described in section~\ref{sec:single-model-no-matrix} is tested by randomly selecting 100 translations from our testset of correct translations. These are used as a known translation, to derive other translations from. For each of these translations, we then select 500 different translations to test against. This process is done both for a model trained at 100 dimensions, and one trained at 400 dimensions, and both for Dutch to English and vice versa.

For example, given "koning $\to$ king" as base pair, we then test whether the algorithm can translate "koningin" to "queen", "kopen" to "buy", et cetera.

Table~\ref{table:results_single_model_no_matrix} shows the results of these experiments.

\begin{table}[ht!]
  \centering
  \label{table:results_single_model_no_matrix}
  \begin{tabular}{ll|r|r|r|r|}
  \cline{3-6}                                       &     & \multicolumn{2}{|c|}{100 dim} & \multicolumn{2}{c|}{400 dim} \\ \cline{3-6} 
                                                    &     & NL$\to$EN   & EN$\to$NL       & NL$\to$EN   & EN$\to$NL      \\ \hline
    \multicolumn{1}{|l|}{\multirow{3}{*}{Avg.}}     & @1  & 4.29\%      & 3.92\%          & 1.91\%      & 2.02\%         \\ \cline{2-6} 
    \multicolumn{1}{|l|}{}                          & @5  & 8.71\%      & 8.28\%          & 4.89\%      & 4.97\%         \\ \cline{2-6} 
    \multicolumn{1}{|l|}{}                          & @10 & 11.4\%      & 10.9\%          & 6.96\%      & 7.00\%         \\ \hline 
    \multicolumn{1}{|l|}{\multirow{3}{*}{Max.}}     & @1  & 14.6\%      & 12.4\%          & 8.60\%      & 8.60\%         \\ \cline{2-6} 
    \multicolumn{1}{|l|}{}                          & @5  & 23.4\%      & 23.8\%          & 15.8\%      & 16.4\%         \\ \cline{2-6} 
    \multicolumn{1}{|l|}{}                          & @10 & 28.4\%      & 28.6\%          & 21.2\%      & 22.0\%         \\ \hline
    \multicolumn{1}{|l|}{\multirow{3}{*}{Stdev.}}   & @1  & 3.50\%      & 3.24\%          & 1.97\%      & 2.19\%         \\ \cline{2-6} 
    \multicolumn{1}{|l|}{}                          & @5  & 6.10\%      & 6.15\%          & 4.11\%      & 4.55\%         \\ \cline{2-6} 
    \multicolumn{1}{|l|}{}                          & @10 & 7.51\%      & 7.60\%          & 5.42\%      & 5.85\%         \\ \hline
  \end{tabular}
  \caption{Translation accuracy, using a single model without translation matrix and 400 dimensions. The minimum is left out, because it is 0.00\% for all scenarios.}
\end{table}

An interesting note (which is not shown in the table), is that for every scenario, the lowest percentage of correct translations is 0.00\%. This means that there are some very bad base translations. In the next section, we will give some reasons what may cause this.

\subsubsection{Using Translation Matrix}
Check figure \ref{fig:accuracy_single_model_wikis} for the results.

Again: weird dip towards the end. \todo{Marc, do you know why? Just bad luck choosing test samples?}

\begin{figure}[ht!]
  \centering \includegraphics[width=\linewidth]{images/accuracy_single_model_wikis}
  \caption{Accuracy for multi-model translations, trained on Dutch and English wiki. 'x' marks denote dimensionality 400, '+' marks dimensionalty 100.}
  \label{fig:accuracy_single_model_wikis}
\end{figure}
>>>>>>> ec439167716fa22c78368379bf9570f1a966101d
