\section{Word2Vec}
\label{sec:word2vec}
Word2vec is an algorithm that computes vector representations of words based on co-occurrences. The spatial distance between two word-vectors corresponds to word similarity. In order to achieve this there two different algorithms: skip-gram and continuous bag of words (CBOW). Skip-gram is the most popular choice because it scales better to bigger datasets and therefore also chosen for our research. 

Skip-gram model is an method for learning distributed vector representations that capture a large number of syntactic and semantic word relationships~\cite{mikolov2013distributed}. Given a a word $w$, the skip-gram model predicts the $n$ neighboring words.

Word2vec is an example of so-called "shallow" learning and can be trained as a simple neural network. This neural network has a single hidden layer with no non-linearities and no unsupervised pre-training of layers is needed~\cite{wang2014introduction}.

Since word2vec is simplified, its scope of application is more limited than deep neural networks~\cite{bengio2007scaling}. However, training is far more efficient due to the lower amount of layers and simpler functions.