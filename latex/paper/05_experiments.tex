\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}
The datasets used to train the word2vec models are freely available online. We used the wikipedia datasets containing a snapshot of all articles on the English and Dutch Wikipedia and a copy of all Reddit comments.

The Go program used for cleaning the Reddit data is published on GitHub\footnote{\gh}. The wikipedia data was parsed with a slightly modified version of Wikipedia Extractor\footnote{\url{https://github.com/bwbaugh/wikipedia-extractor}}, which is also available on our GitHub page.

The characteristics of the cleaned datasets can be seen in table~\ref{table:datasets}.

%
% Get these statistics with:
% wc -w <files>		for the wordcount. Do it on the extracted, processed text (i.e. the input for word2vec)
% Number of items: see end of running wikiextractor.py
% e.g. INFO: Finished 31-process extraction of 1831031 articles in 11250.7s (162.7 art/s)
%
\begin{table}[ht!]
	\centering
	\label{table:datasets}
	\begin{tabular}{|l|c|r|r|}
	\hline
	Name																												& Items			& Words			\\
	\hline
	Reddit comments \tablefootnote{\url{http://academictorrents.com/details/7690f71ea949b868080401c749e878f98de34d3d}} 	& 1,325,482,268 & 38,177,224,313\\
	English Wikipedia \tablefootnote{\url{https://dumps.wikimedia.org/enwiki/20150901/}}								& 4,929,936		& 1,707,791,444	\\
	Dutch Wikipedia \tablefootnote{\url{https://dumps.wikimedia.org/nlwiki/20150901/}}									& 1,831,031		& 209,095,532	\\
	\hline
	\end{tabular}
	\caption{Dataset statistics. For Wikipedia, "Items" refers to the number of articles. For Reddit, it refers to the number of comments.}
\end{table}

\subsection{Tests}
We tested each translation method in multiple configurations, most notably with different dimensions for the language models.

The accuracy of the translation is measured in percentages how often the algorithm gave the expected answer. To account for answers that are almost correct, we keep the following statistics:
\begin{itemize}
	\item \textbf{Accuracy @1}: the algorithm gave the correct answer.
	\item \textbf{Accuracy @5}: the correct answer was in the top 5.
	\item \textbf{Accuracy @10}: the correct answer was in the top 10.
<<<<<<< HEAD
\end{itemize}
=======
\end{itemize}

If the answer is in the top 5 it is likely to be a synonym or very related word, since word2vec tends to group words by meaning. For example, the translation may be "buy" or "acquire" where the correct translation is "purchase".
>>>>>>> ec439167716fa22c78368379bf9570f1a966101d
