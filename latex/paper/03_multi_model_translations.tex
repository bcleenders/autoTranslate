\section{Multi-model Translations}
\label{sec:multi-model-translations}
In this section, we explain a technique for translating words using seperate models for the input and output language. The techniques described here are originally published by Mikolov et al.~\cite{mikolov2013exploiting}.

\subsection{Translation}
Given a word in language A that we want to translate to language B, the first step is to find the vector representation of the word. We do this by looking up the word in a word2vec model trained on language A.

Translating is now a matter of mapping vector representations from model A to corresponding vectors in model B. We do this by multiplying the vector with a translation matrix, which gives an expected vector in model B.

The last step is to convert the "translated" vector representation back to a word. We do this by looking up which word in language B which has the vector representation closest to the translated vector. The criterium used for this is simply nearest neighbour with a Euclidean distance.

\subsection{Training the Models}
This way of translation requires three models: word2vec models for both language A and B and a translation matrix from A to B.

The two language models are trained using the default word2vec algorithm. We refer to section~\ref{sec:word2vec} and previous papers for the specifics on this \cite{mikolov2013efficient, mikolov2013distributed}.

The translation matrix is trained by solving the following expression:
$$ \argmin_{T} \Sigma_{i=1}^{n} || T \cdot a_i - b_i || ^{2}$$
where $T$ is an $n$ by $n$ matrix, $a_i$ and $b_i$ are $n$ dimensional vector representations of words in languages A and B, such that $b_i$ is the translation of $a_i$.

The quality of the translation largely depends on the accuracy of the mapping from vectors in model A to model B. Training language models using word2vec is unsupervised, and can therefore use hundreds of gigabytes or even terabytes of training data. Training the translation matrix is a supervised process (you have to provide correct translations), which makes it impractical to provide more than a few thousand words.

An advantage of using this method, is that it not only provides a word translation but also gives a distance between the translated vector and its nearest neighbour. If this distance is large, it indicates a higher level of uncertainty in the matrix. As such, one can search for translations where the algorithm is unsure what to choose and provide targeted training data to improve a next iteration of the model.

\subsection{Training models of different sizes}
One of the parameters when training word2vec models is the number of dimensions to train on. Typically, the number of dimensions is between 100 and 400\cite{mikolov2013efficient}.

Large datasets contain many relations, and can be trained on high dimensions (400 or more), but small datasets (i.e. under 50 million words) tend to be trained with lower dimensionality. However, these numbers are not set in stone: a study by Pennington et al.~\cite{jeffreypennington2014glove} indicates that 300 dimensions sufficiently capture relations -for their dataset. Since training complexity scales linearly with the number of dimensions, for very large datasets a high dimensionality might even simply be too costly.

Having discussed the cost/benefit of higher dimensions, we remark that translations with multiple models also allows for models trained with a different number of dimensions. If both models have the same dimensions, the translation matrix will be square. If the dimensions are different, one can either use dimensionality reduction on the larger matrix, or use a non-square translation matrix to scale down the dimensionality.